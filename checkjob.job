#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=2
#SBATCH --job-name=Phi3_COT_relation
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=1:00:00
#SBATCH --output=job_outputs/Phi3_checkjob_slurm_output_%A.out

module purge
module load 2022
module load Anaconda3/2022.05

# Activate your environment
source activate ATCS-project
# srun python Project/inference.py --model "berkeley-nest/Starling-LM-7B-alpha" --tokenizer "berkeley-nest/Starling-LM-7B-alpha"
# --datasets (default: 'BATS') --BATS_filename --prompt_structure (default: 'by-relation'))  --COT_template (default:False) --number_of_analogy (default:all)
srun python run.py --dataset "BATS" --BATS_filename "L01 [hypernyms - animals]" --number_of_analogy 2
srun python run.py --dataset "BATS" --BATS_filename "L01 [hypernyms - animals]" --prompt_structure "by-target-word" --number_of_analogy 2
srun python run.py --dataset "BATS" --BATS_filename "L01 [hypernyms - animals]" --COT_template "Thinking step by step." --number_of_analogy 2
srun python run.py --dataset "BATS" --BATS_filename "L01 [hypernyms - animals]" --prompt_structure "by-target-word" --COT_template "Thinking step by step." --number_of_analogy 1

# for few shot
# python run.py --dataset BATS-fewshot --BATS_filename 'L07 [synonyms - intensity] sample' --number_of_shot 10 --number_of_analogy 4
# python run.py --dataset BATS-fewshot --BATS_filename 'L07 [synonyms - intensity] sample' --number_of_shot 10 --number_of_analogy 4 --COT_template " Thinking step by step. " --explanation 'short'