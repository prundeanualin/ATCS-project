{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EryM1iOO1tus"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqHxFIH7ZU92"
      },
      "source": [
        "Download all the necessary dependencies. These should be exactly the ones present in the `environment.yaml` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qd077LDZZU92"
      },
      "outputs": [],
      "source": [
        "!pip -q install numpy tqdm pandas transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qzNM-x5ap1H",
        "outputId": "170fa996-76c1-4163-8a67-69bff819dd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ATCS-project' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/prundeanualin/ATCS-project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6_Aiv01ZlsxH"
      },
      "outputs": [],
      "source": [
        "# IF YOU WANT TO TEST THINGS FROM YOUR OWN BRANCH, UNCOMMENT BELOW\n",
        "# ! git checkout <your_own_branch>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUkx5dxwLbig",
        "outputId": "f48c22de-958f-4d26-e065-10aaa5be655d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "! git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK8Ichtqbo0-",
        "outputId": "44f9d8ee-210a-4194-b4f3-9c684b10c91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ATCS-project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ATCS-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9t8QNjnV1sE9"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "from get_datasets import SCAN_EXAMPLES_FILEPATH, EXAMPLE_CATEGORIES\n",
        "from prompt_templates.analogy import ANALOGY_TEMPLATE_SIMPLE_INFERENCE, ANALOGY_TEMPLATE_SIMPLE_FULL\n",
        "from model import LLMObj\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import BitsAndBytesConfig\n",
        "import pickle\n",
        "from datasets import ScanDataset\n",
        "import os\n",
        "\n",
        "from utils import seed_experiments, SAVE_DIR\n",
        "\n",
        "os.environ['HF_TOKEN'] = \"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\"\n",
        "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
        "\n",
        "torch.set_default_device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_pv2citJXk0"
      },
      "source": [
        "# Inference\n",
        "\n",
        "`LLMObj` is a HF wrapper that contains the LLM model, tokenizer, and text generation wrapper.\n",
        "\n",
        "Below the class code, several LLMs that are available on HF are initialized.\n",
        "\n",
        "For some models like LLama, you need to authenticate your HF account, so add your [HF access token](https://huggingface.co/docs/hub/security-tokens) to the secrets on secrets as `HF_TOKEN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JovzZZmndZwU"
      },
      "source": [
        "## Model arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bpLE9t0Hc87f"
      },
      "outputs": [],
      "source": [
        "# Since ArgParser does not work in colab, we just construct a custom class with all our neccessary arguments\n",
        "class Args(argparse.Namespace):\n",
        "  model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "  tokenizer = None\n",
        "  quantization = \"4bit\"\n",
        "  low_cpu_mem_usage = True\n",
        "  seed=1234\n",
        "\n",
        "args = Args()\n",
        "\n",
        "seed_experiments(args.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSGeVnOGiyOU"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5J4-r3diz00",
        "outputId": "736f21a4-914e-48e7-f861-6f0651d1188a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCAN datasets already downloaded.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = ScanDataset(\n",
        "    shuffle=False,\n",
        "    analogy_sentence_infer=ANALOGY_TEMPLATE_SIMPLE_INFERENCE,\n",
        "    analogy_sentence_full=ANALOGY_TEMPLATE_SIMPLE_FULL,\n",
        "    examples_file=SCAN_EXAMPLES_FILEPATH.format(EXAMPLE_CATEGORIES[0]),\n",
        "    examples_start_idx=0,\n",
        "    examples_shot_nr=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWUSy3rdc7T"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1t02OFsdZU93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "486ceedbd0154ae5b680067ff6952237",
            "9a244b2c789f495b9f619df718b78b0a",
            "dd5e0e8f09cb41598d837673d406bf1c",
            "e609afd046164e73986f4b285386013e",
            "4a278c04f46d47458aaf410615bcda8a",
            "6b99ca87672f4ed18a6fccb63621bd82",
            "14fdda7bb9174e1ca3a8dcc236b8d813",
            "49988d38783540e5a57fc2693bb42009",
            "29d0fba55d374d7987c5269170b56dc7",
            "758858232840495c8621445d4b2abac0",
            "e56b6f87ed184a8ab1e6491c4e2b2c3d"
          ]
        },
        "outputId": "9bdad1f5-16b0-4615-a400-1db82a11be38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMObj Arguments are:\n",
            "{'model': 'microsoft/Phi-3-mini-4k-instruct', 'model_kwargs': {'torch_dtype': torch.bfloat16, 'low_cpu_mem_usage': True, 'quantization_config': BitsAndBytesConfig {\n",
            "  \"_load_in_4bit\": true,\n",
            "  \"_load_in_8bit\": false,\n",
            "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
            "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "  \"bnb_4bit_quant_type\": \"fp4\",\n",
            "  \"bnb_4bit_use_double_quant\": false,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": null,\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            "}, 'tokenizer_name': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.8f5f3a02ec472594e949c39f8e38c7be8d983bcd.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.8f5f3a02ec472594e949c39f8e38c7be8d983bcd.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "486ceedbd0154ae5b680067ff6952237"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ----- Prepare model arguments -----\n",
        "quantization = None\n",
        "if args.quantization == '4bit':\n",
        "    quantization = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model_kwargs = {\n",
        "    \"torch_dtype\": torch.bfloat16,\n",
        "    \"low_cpu_mem_usage\": args.low_cpu_mem_usage,\n",
        "    \"quantization_config\": quantization\n",
        "}\n",
        "LLMObj_args = {\n",
        "    'model': args.model,\n",
        "    'model_kwargs': model_kwargs,\n",
        "    'tokenizer_name': args.tokenizer\n",
        "}\n",
        "print(\"LLMObj Arguments are:\")\n",
        "print(LLMObj_args)\n",
        "\n",
        "# ----- Load the model -----\n",
        "LLM = LLMObj(**LLMObj_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y0wLh-zfLpu"
      },
      "source": [
        "## Run the inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QdH8LLgSfOYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d186f2-bfe9-4411-b766-23c51775445e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompting with: \n",
            "If heat transfer is like water flow, then cooling is like...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:39, 39.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is:\n",
            " If heat transfer is likened to water flow, then cooling is like diverting the water flow or reducing its flow rate. Just as you can control the amount of water flowing through a pipe, you can control the rate of heat transfer by manipulating the conditions that affect it. This can be done through various methods such as:\n",
            "\n",
            "1. Increasing the surface area for heat exchange: This is similar to widening the pipe to allow more water to flow through. By increasing the surface area of an object exposed to cooling, more heat can be dissipated.\n",
            "\n",
            "2. Enhancing heat transfer coefficients: This is akin to increasing the water pressure or using a pump to increase the water flow rate. In heat transfer, this can be achieved by using materials with higher thermal conductivity or by improving the heat transfer fluid's properties (e.g., using a coolant with better heat absorption capabilities).\n",
            "\n",
            "3. Improving heat transfer mechanisms: This is like optimizing the water flow system by using more efficient pumps or pipes. In cooling, this can be achieved by using more effective cooling methods, such as forced convection, natural convection, or using heat exchangers.\n",
            "\n",
            "4. Controlling the temperature gradient: This is similar to regulating the water pressure or flow rate. In cooling, this can be done by controlling the temperature difference between the object being cooled and the cooling medium (e.g., air, water, or refrigerant).\n",
            "\n",
            "5. Insulation: This is like insulating the pipes to prevent heat loss. In cooling, insulation can be used to minimize heat gain from the surroundings, thus reducing the cooling load.\n",
            "\n",
            "In summary, cooling is like managing the water flow to ensure that the desired temperature is achieved and maintained. By manipulating the factors that affect heat transfer, you can effectively control the cooling process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Stop at just stop_at_datapoint_idx generations, just to see it in action\n",
        "stop_at_datapoint_idx = 1\n",
        "\n",
        "\n",
        "# LLM.update_system_prompt(\"Always answer in French\")\n",
        "# print(\"New system prompt is\")\n",
        "# print(LLM.chat_template[0]['content'])\n",
        "\n",
        "# Run inference\n",
        "results = []\n",
        "for i, sample in tqdm(enumerate(dataset)):\n",
        "  if i >= stop_at_datapoint_idx:\n",
        "    break\n",
        "\n",
        "  sample = dataset[10]\n",
        "  inference_extended = sample['inference']\n",
        "\n",
        "  print(\"Prompting with: \")\n",
        "  print(inference_extended)\n",
        "\n",
        "  output = LLM.generate(inference_extended)\n",
        "\n",
        "  print(\"Output is:\")\n",
        "  print(output)\n",
        "\n",
        "  results.append([sample, output])\n",
        "\n",
        "# if os.path.exists(f'{args.model.split(\"/\")[1]}_generated_prompts.pl'):\n",
        "#   print(\"File exists!!\")\n",
        "\n",
        "save_file = os.path.join(SAVE_DIR, f'{args.model.split(\"/\")[1]}_generated_prompts.pl')\n",
        "with open(save_file, 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-gv3uQ9laYV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b228ec3-a3de-4f2f-f1b7-d490eae44ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score is 100.0%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from evaluate import *\n",
        "\n",
        "# results = pd.read_pickle(f'{args.model.split(\"/\")[1]}_generated_prompts.pl')\n",
        "\n",
        "# ----- Evaluate -----\n",
        "\n",
        "# results = pd.read_pickle(f'{args.model.split(\"/\")[1]}_generated_prompts.pl')\n",
        "\n",
        "acc_score = evaluate(results, SimpleEvaluationStrategy())\n",
        "print(f\"Score is {acc_score}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "486ceedbd0154ae5b680067ff6952237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a244b2c789f495b9f619df718b78b0a",
              "IPY_MODEL_dd5e0e8f09cb41598d837673d406bf1c",
              "IPY_MODEL_e609afd046164e73986f4b285386013e"
            ],
            "layout": "IPY_MODEL_4a278c04f46d47458aaf410615bcda8a"
          }
        },
        "9a244b2c789f495b9f619df718b78b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b99ca87672f4ed18a6fccb63621bd82",
            "placeholder": "​",
            "style": "IPY_MODEL_14fdda7bb9174e1ca3a8dcc236b8d813",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dd5e0e8f09cb41598d837673d406bf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49988d38783540e5a57fc2693bb42009",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29d0fba55d374d7987c5269170b56dc7",
            "value": 2
          }
        },
        "e609afd046164e73986f4b285386013e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_758858232840495c8621445d4b2abac0",
            "placeholder": "​",
            "style": "IPY_MODEL_e56b6f87ed184a8ab1e6491c4e2b2c3d",
            "value": " 2/2 [00:27&lt;00:00, 13.84s/it]"
          }
        },
        "4a278c04f46d47458aaf410615bcda8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b99ca87672f4ed18a6fccb63621bd82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14fdda7bb9174e1ca3a8dcc236b8d813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49988d38783540e5a57fc2693bb42009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d0fba55d374d7987c5269170b56dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "758858232840495c8621445d4b2abac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56b6f87ed184a8ab1e6491c4e2b2c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}