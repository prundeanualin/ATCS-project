{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EryM1iOO1tus"
   },
   "source": "### Imports & Setup"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a separate conda environment from the environment.yaml file.\n",
    "\n",
    "`conda env create -f environment.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t8QNjnV1sE9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import accelerate\n",
    "import numpy as np\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline\n",
    "                          )\n",
    "import textwrap\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Class and LLM Checking\n",
    "\n",
    "`LLMObj` is a HF wrapper that contains the LLM model, tokenizer, and text generation wrapper.\n",
    "\n",
    "Below the class code, several LLMs that are available on HF are initialized.\n",
    "\n",
    "For some models like LLama, you need to authenticate your HF account, so add your [HF access token](https://huggingface.co/docs/hub/security-tokens) to the secrets on secrets as `HF_TOKEN`."
   ],
   "metadata": {
    "id": "D_pv2citJXk0"
   }
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import LLMObj, parse_args\n",
    "\n",
    "args = parse_args()\n",
    "print(args)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qzQBdM7_XeB"
   },
   "source": "### LLM Inference"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi4jznFuRMYV"
   },
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ero_7LEg_XeB"
   },
   "outputs": [],
   "source": [
    "# use the commented out parts for running in 4bit\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "model_kwargs = {\"torch_dtype\": torch.bfloat16,\n",
    "            \"quantization_config\": quantization_config,\n",
    "            \"low_cpu_mem_usage\": True}\n",
    "\n",
    "Llama = LLMObj(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K3kF8jhu1se"
   },
   "outputs": [],
   "source": [
    "Llama.generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "        #  system_prompt=\"You are a helpful assistant called Llama-3. Write out your reasoning step-by-step to be sure you get the right answers!\",\n",
    "         max_length=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9QimeuOROTI"
   },
   "source": [
    "### Starling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uHyiy64R0LiM"
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_kwargs = {\"torch_dtype\": torch.bfloat16,\n",
    "            \"quantization_config\": quantization_config\n",
    "            }\n",
    "\n",
    "starling = LLMObj(model=\"berkeley-nest/Starling-LM-7B-alpha\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD4RtGp-7lWI"
   },
   "source": [
    "### Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBAOyjU37ndt"
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_kwargs = {\"torch_dtype\": torch.bfloat16,\n",
    "            \"quantization_config\": quantization_config\n",
    "            }\n",
    "\n",
    "phi_3 = LLMObj(model=\"microsoft/Phi-3-mini-128k-instruct\", model_kwargs=model_kwargs, tokenizer=\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3P6U1x0RSZL"
   },
   "source": [
    "## Dataloader and analogy template sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analogy sentences\n",
    "\n",
    "This is the template used for constructing the analogy sentence. Here we have the analogy sentence with the missing word that needs to be inferred, along with the full analogy sentence that is provided as an example."
   ],
   "metadata": {
    "id": "xoiCatdFe3_U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ANALOGY_TEMPLATE_SIMPLE_FULL = \"If {} is like {}, then {} is like {}.\"\n",
    "ANALOGY_TEMPLATE_SIMPLE_INFERENCE = \"If {} is like {}, then {} is like...\""
   ],
   "metadata": {
    "id": "1t9z8arlfAbk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the datasets\n",
    "- Downloading SCAN dataset and examples"
   ],
   "metadata": {
    "id": "73CmRCYwdaOg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SCAN_DATASET_FILEPATH = \"scan.csv\"\n",
    "SCAN_EXAMPLES_FILEPATH = \"scan_examples.txt\"\n",
    "\n",
    "url_scan_dataset = \"https://raw.githubusercontent.com/prundeanualin/ATCS-project/blob/main/data/SCAN/SCAN_dataset.csv\"\n",
    "    response = requests.get(url_scan_dataset)\n",
    "    if response.status_code == 200:\n",
    "        csv_content = response.text\n",
    "        with open(SCAN_DATASET_FILEPATH, \"w\") as csv_file:\n",
    "            csv_file.write(csv_content)\n",
    "\n",
    "        print(\"SCAN dataset file downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(\"Failed to download SCAN dataset file. Status code:\", response.status_code)\n",
    "\n",
    "url_scan_examples = \"https://raw.githubusercontent.com/prundeanualin/ATCS-project/blob/main/data/SCAN/SCAN_examples.txt\"\n",
    "    response = requests.get(url_scan_examples)\n",
    "    if response.status_code == 200:\n",
    "        csv_content = response.text\n",
    "        with open(SCAN_EXAMPLES_FILEPATH, \"w\") as csv_file:\n",
    "            csv_file.write(csv_content)\n",
    "\n",
    "        print(\"SCAN examples file downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(\"Failed to download SCAN examples file. Status code:\", response.status_code)"
   ],
   "metadata": {
    "id": "JV53gydtdf0Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the dataset"
   ],
   "metadata": {
    "id": "ugsqZCWLdcY3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_list_alternatives(alternatives):\n",
    "    if alternatives == 'nan':\n",
    "        return []\n",
    "    else:\n",
    "        return alternatives.split(', ')\n",
    "\n",
    "\n",
    "class ScanDataLoader():\n",
    "    def __init__(self,\n",
    "                 shot_nr=1,\n",
    "                 examples_start_idx=0,\n",
    "                 analogy_sentence_infer=ANALOGY_TEMPLATE_SIMPLE_INFERENCE,\n",
    "                 analogy_sentence_full=ANALOGY_TEMPLATE_SIMPLE_FULL):\n",
    "      \"\"\"\n",
    "        - examples_start_idx: needs to be within the range of available examples per analogy type.\n",
    "      \"\"\"\n",
    "\n",
    "        self.shot_nr = shot_nr\n",
    "        self.examples_start_idx = examples_start_idx\n",
    "        self.analogy_sentence_inference = analogy_sentence_infer\n",
    "        self.analogy_sentence_example = analogy_sentence_full\n",
    "\n",
    "        with open(SCAN_DATASET_FILEPATH, 'r', encoding='utf8') as f:\n",
    "            self.df = pd.read_csv(f, sep=',', index_col=False)\n",
    "\n",
    "        # Transform alternatives into list of strings. If none is provided, then use an empty list\n",
    "        self.df['alternatives'] = self.df['alternatives'].astype(str).map(get_list_alternatives)\n",
    "\n",
    "        # there were still some rows with same first three words but different fourth word. This should not\n",
    "        # be the case since then the second instance should be part of the first's alternatives\n",
    "        last_doubled = self.df[self.df.duplicated(subset=['source', 'target', 'targ_word'], keep='last')]\n",
    "        self.df.drop(last_doubled.index, axis=0, inplace=True)\n",
    "\n",
    "        for _, row in last_doubled.iterrows():\n",
    "            first = self.df[(self.df['source'] == row['source']) & (self.df['target'] == row['target']) & (\n",
    "                    self.df['targ_word'] == row['targ_word'])].index\n",
    "\n",
    "            self.df.at[first.values[0], 'alternatives'] = self.df.at[first.values[0], 'alternatives'] + row[\n",
    "                'alternatives'] + [row['src_word']]\n",
    "        self.examples = {\n",
    "            'science': [],\n",
    "            'metaphor': []\n",
    "        }\n",
    "\n",
    "        with open(SCAN_EXAMPLES_FILEPATH, 'r', encoding='utf8') as f:\n",
    "            lines = [line.rstrip() for line in f]\n",
    "            nr_examples = round(len(lines) / 3)\n",
    "            examples = [lines[idx * 3: (idx * 3) + 2] for idx in range(0, nr_examples)]\n",
    "            for example in examples:\n",
    "                target, source, targ_word, src_word, alternatives, analogy_type = tuple(example[0].split(','))\n",
    "                ex = {\n",
    "                    'target': target,\n",
    "                    'source': source,\n",
    "                    'targ_word': targ_word,\n",
    "                    'src_word': src_word,\n",
    "                    'detailed_cot': example[1],\n",
    "                    'simple': self.analogy_sentence_example.format(target, source, targ_word, src_word),\n",
    "                    'analogy_type': analogy_type\n",
    "                }\n",
    "                self.examples[analogy_type].append(ex)\n",
    "\n",
    "        self.current_examples = None\n",
    "        self.df_remapped_indices = None\n",
    "        set_current_examples_and_exclude_from_dataset(self.examples_start_idx, self.shot_nr)\n",
    "\n",
    "\n",
    "  def set_current_examples_and_exclude_from_dataset(examples_start_idx, nr_examples):\n",
    "        # Get the nr_examples of examples starting from the start_idx, for each analogy type\n",
    "        self.current_examples = []\n",
    "        for k in  self.examples.keys():\n",
    "          self.current_examples.extend(self.examples[k][self.examples_start_idx: self.shot_nr])\n",
    "\n",
    "        # Remapping the indices in df so that those corresponding to the examples_to_consider are removed.\n",
    "        # This way, the examples will not be returned when iterating through the df\n",
    "        self.df_remapped_indices = [i for i in range(len(self.df))]\n",
    "        indices_examples = []\n",
    "        for ex in self.current_examples:\n",
    "            idx = self.df[(self.df['source'] == ex['source']) & (self.df['target'] == ex['target']) & (\n",
    "                    self.df['targ_word'] == ex['targ_word']) & (self.df['targ_word'] == ex['targ_word'])].index\n",
    "            indices_examples.append(idx.values[0])\n",
    "        for ex_i in indices_examples:\n",
    "            self.df_remapped_indices.pop(ex_i)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.df))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        idx = self.df_remapped_indices[idx]\n",
    "        analogy_sent = self.df.iloc[idx, :3].values.tolist()\n",
    "        label = self.df.iloc[idx, 3]\n",
    "        alternatives = self.df.iloc[idx, 4]\n",
    "        analogy_type = self.df.iloc[idx, 5]\n",
    "\n",
    "        return {\n",
    "            'inference': self.analogy_sentence_inference.format(*analogy_sent),\n",
    "            'examples': self.current_examples,\n",
    "            'label': label,\n",
    "            'alternatives': alternatives,\n",
    "            'analogy_type': analogy_type\n",
    "        }"
   ],
   "metadata": {
    "id": "rBYieFB3dmSb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jNQ4mVoNPfx"
   },
   "outputs": [],
   "source": [
    "# class SCANDataloader:\n",
    "#   def __init__(self):\n",
    "#     url = \"https://raw.githubusercontent.com/taczin/SCAN_analogies/main/data/SCAN_dataset.csv\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         csv_content = response.text\n",
    "#         with open(\"scan.csv\", \"w\") as csv_file:\n",
    "#             csv_file.write(csv_content)\n",
    "\n",
    "#         print(\"CSV file downloaded successfully.\")\n",
    "#     else:\n",
    "#         print(\"Failed to download CSV file. Status code:\", response.status_code)\n",
    "\n",
    "#     self.data = pd.read_csv('scan.csv')\n",
    "#     self.data.fillna(\"NaN\", inplace=True)\n",
    "#     self.shuffled_idx = np.arange(len(self.data))\n",
    "#     # np.random.shuffle(self.shuffled_idx)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     row = self.data.iloc[self.shuffled_idx[idx]]\n",
    "#     prompt = f\"If {row['source']} is like {row['target']}, then {row['src_word']} is like...\"\n",
    "#     return {\"row\": row.to_dict(), \"prompt\": prompt}\n",
    "\n",
    "#   def __call__(self):\n",
    "#     for idx in self.shuffled_idx:\n",
    "#       row = self.data.iloc[self.shuffled_idx[idx]]\n",
    "#       prompt = f\"If {row['source']} is like {row['target']}, then {row['src_word']} is like...\"\n",
    "#       yield {\"row\": row.to_dict(), \"prompt\": prompt}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qH6Zb2FRa_t"
   },
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_sp2uBNRdSY"
   },
   "outputs": [],
   "source": [
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "# model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "# model_kwargs = {\"torch_dtype\": torch.bfloat16,\n",
    "#             \"quantization_config\": quantization_config,\n",
    "#             \"low_cpu_mem_usage\": True}\n",
    "\n",
    "# Llama = LLMObj(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs=model_kwargs)\n",
    "\n",
    "SCAN_loader = ScanDataLoader()\n",
    "# ds = Dataset.from_generator(SCAN_loader)\n",
    "#   from tqdm import tqdm\n",
    "generated_prompts = []\n",
    "\n",
    "for i, sample in tqdm(enumerate(SCAN_loader)):\n",
    "  output = phi_3.generate(sample['inference'])\n",
    "  generated_prompts.append([sample, output])\n",
    "  # if i > 5:\n",
    "  #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiEWePOTXq_O"
   },
   "outputs": [],
   "source": [
    "def process_line(item, llm):\n",
    "  # Generate the prompt\n",
    "  prompt = f\"If {item['source']} is like {item['target']}, then {item['src_word']} is like...\"\n",
    "  # Generate the response\n",
    "  response = llm.generate(prompt)\n",
    "  # Return the complete line with the response\n",
    "  return {**item, 'response': response}\n",
    "\n",
    "with open('scan_analogy_responses.csv', 'w') as f:\n",
    "    headers = ['target', 'source', 'targ_word', 'src_word', 'alternatives', 'analogy_type', 'response']\n",
    "    writer = csv.DictWriter(f, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Read and process each line\n",
    "    for chunk in pd.read_csv('scan.csv', chunksize=1):\n",
    "        for index, row in chunk.iterrows():\n",
    "            result = process_line(row, llm)\n",
    "            writer.writerow(result)\n",
    "\n",
    "print(\"Data with generated responses has been saved\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "EryM1iOO1tus",
    "qi4jznFuRMYV"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
